{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMEN - Convocatoria 1 - Programación\n",
    "Utilizar el conjunto de datos \"dataset_exam.npy\" para resolver el ejercicio. Tener en cuenta que la última columna corresponde a la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics \n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset_exam.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mdataset_exam.npy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m X \u001b[39m=\u001b[39m dataset[:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m      3\u001b[0m Y \u001b[39m=\u001b[39m dataset[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset_exam.npy'"
     ]
    }
   ],
   "source": [
    "dataset = np.load('dataset_exam.npy')\n",
    "X = dataset[:,:-1]\n",
    "Y = dataset[:,-1]\n",
    "print(np.unique(Y, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Partición de datos externa (1 punto)\n",
    "Realizar una partición externa de tipo hold-out seleccionando un 20% de los datos para test (fijar una semilla en 42).\n",
    "Comprobad si los datos, tanto de train como de test, están más o menos balanceados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([195, 198]))\n",
      "(array([0., 1.]), array([51, 48]))\n"
     ]
    }
   ],
   "source": [
    "# realizar partición externa\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# comprobar que están balanceados\n",
    "print(np.unique(Y_train, return_counts=True))\n",
    "print(np.unique(Y_test, return_counts=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Estandarización de los datos de train y test (1 punto)\n",
    "Utilizar el método StandardScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estandarización\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Selección de atributos en train y test (1 punto)\n",
    "Aplicar el método de mutual information para clasificación seleccionando un percentile=90. (Recordad la función \"SelectPercentile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(393, 8)\n",
      "(393, 7)\n"
     ]
    }
   ],
   "source": [
    "selector = SelectPercentile(mutual_info_classif, percentile=90)\n",
    "X_train_selected = selector.fit_transform(X_train_std, Y_train)\n",
    "X_test_selected = selector.fit_transform(X_test_std, Y_test)\n",
    "print(X_train_std.shape)\n",
    "print(X_train_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Comparación de modelos de clasificación mediante validación cruzada (3 puntos)\n",
    "Instrucciones:\n",
    "\n",
    "- Aplicar una validación cruzada interna de K=5 bolsas para optimizar y comparar la capacidad predictiva de los siguientes modelos: Regresión Logística y Support Vector Machine.\n",
    "- La optimización de hiperparámetros debe realizarse de manera automática. (Recordad la función \"GridSearchCV\").\n",
    "- La comparación debe realizarse únicamente en términos de exactitud proporcionando resultados de media +- desviación estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGR: Exactitud: 0.6765660499837715 +- 0.05376587102746887\n",
      "SVM: Exactitud: 0.6841934436871145 +- 0.04911119734717438\n"
     ]
    }
   ],
   "source": [
    "algoritmos = { \"LOGR\": LogisticRegression(random_state=42),\n",
    "              \"SVM\": SVC(random_state=42)\n",
    "         }\n",
    "scores = {}\n",
    "bag_size = 5\n",
    "scorer = metrics.make_scorer(metrics.accuracy_score)\n",
    "for name, m in algoritmos.items():\n",
    "    scores[name] = cross_val_score(m, X_train_selected, Y_train, cv=KFold(n_splits=bag_size, shuffle=True, random_state=42), scoring=scorer)\n",
    "    print(f\"{name}: Exactitud: {scores[name].mean()} +- {scores[name].std()}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Evaluación de los modelos sobre el conjunto de test (2.5 puntos)\n",
    "- Entrenar los modelos anteriores utilizando todos los datos de entrenamiento.\n",
    "- Evaluar su rendimiento sobre el conjunto de test mostrando una tabla de resultados tal que:\n",
    " * Las filas serán: Precisión, Sensibilidad, F-score, Exactitud y AUC\n",
    " * Las columnas serán: LOGR y SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGR\n",
      "SVM\n",
      "METRIC LOGR SVM\n",
      "PRECISION 0.7070707070707071 0.7171717171717171\n",
      "SENSIBILIDAD 0.7070707070707071 0.7171717171717171\n",
      "F1SCORE 0.7070707070707071 0.7171717171717171\n",
      "EXACTITUD 0.7070707070707071 0.7171717171717171\n",
      "AUC 0.7077205882352942 0.7138480392156862\n"
     ]
    }
   ],
   "source": [
    "ypred = {}\n",
    "metricas = { \"PRECISION\": lambda y_true, y_pred:\n",
    "            metrics.precision_score(y_true, y_pred, average='micro'),\n",
    "            \"SENSIBILIDAD\": lambda y_true, y_pred:\n",
    "            metrics.recall_score(y_true, y_pred, average='micro'),\n",
    "            \"F1SCORE\":lambda y_true, y_pred:\n",
    "            metrics.f1_score(y_true, y_pred, average='micro'),\n",
    "            \"EXACTITUD\": metrics.accuracy_score,\n",
    "            \"AUC\": metrics.roc_auc_score}\n",
    "metrics_per_alg = {}\n",
    "ypred = {}\n",
    "\n",
    "for name, alg in algoritmos.items():\n",
    "    print(name)\n",
    "    modelo = alg.fit(X_train_selected, Y_train)\n",
    "    ypred[name] = modelo.predict(X_test_selected)\n",
    "    results = {}\n",
    "    for metric_name, m in metricas.items():\n",
    "        results[metric_name] = m(Y_test, ypred[name])\n",
    "    metrics_per_alg[name] = results\n",
    "\n",
    "\n",
    "print(\"METRIC\", \"LOGR\", \"SVM\")\n",
    "for key, _ in metrics_per_alg[\"LOGR\"].items():\n",
    "    print(key, metrics_per_alg[\"LOGR\"][key], metrics_per_alg[\"SVM\"][key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Visualización de resultados (1 punto)\n",
    "- Mostrar la matriz de confusión\n",
    "- Mostrar en una única figura la comparación de las curvas ROC obtenidas por cada modelo. (Recordad que SVM requiere un parámetro específico en su llamada para poder calcular la curva ROC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35 16]\n",
      " [13 35]]\n",
      "[[42  9]\n",
      " [19 29]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(Y_test, ypred[\"LOGR\"]))\n",
    "print(metrics.confusion_matrix(Y_test, ypred[\"SVM\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Interpretación de resultados (0.5 puntos)\n",
    "* Justifica brevemente cuál de los dos modelos utilizarías para ponerlo en producción"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
