{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tEn este ejercicio trabajaremos con el método de descenso de gradiente, el cual constituye otra herramienta crucial, en esta ocasión de la rama del cálculo, para el proceso de retropropagación asociado al entrenamiento de una red neuronal.\n",
    "\n",
    "a) Prográmese en Python el método de descenso de gradiente para funciones de\n",
    "n variables. La función deberá tener como parámetros de entradas:\n",
    "- El gradiente de la función que se desea minimizar ∇f (puede venir dada como otra función previamente implementada, grad f, con entrada un vector, representando el punto donde se quiere calcular el gradiente, y salida otro vector, representando\n",
    "el gradiente de f en dicho punto).\n",
    "- Un valor inicial x0 ∈ Rn (almacenado en un vector de n componentes). El ratio de aprendizaje γ (que se asume constante para cada iteración).\n",
    "- Un parámetro de tolerancia tol (con el que finalizar el proceso cuando ∥∇f (x)∥2 <\n",
    "tol).\n",
    "- Un número máximo de iteraciones maxit (con el fin de evitar ejecuciones indefinidas en caso de divergencia o convergencia muy lenta).\n",
    "- La salida de la función deberá ser la aproximación del x que cumple f ′(x) ≈ 0, correspondiente a la última iteración realizada en el método.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(grad_f, x0, gamma, tol, maxit):\n",
    "    x = x0\n",
    "    for i in range(maxit):\n",
    "        gradient = grad_f(x)\n",
    "        if abs(gradient) < tol:\n",
    "            break\n",
    "        x = x - gamma * gradient\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\tSea la función f : R → R dada por f (x) = 3x4 + 4x3 − 12x2 + 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Aplica el m´etodo sobre f(x) con x0 = 3 γ = 0.001, tol=1e-12, maxit=1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado 1: 1.0000000000000275\n"
     ]
    }
   ],
   "source": [
    "#Primero, definiremos la función grad_f que calculará el gradiente de f(x):\n",
    "def grad_f(x):\n",
    "    return 12*x**3 + 12*x**2 - 24*x\n",
    "\n",
    "# Aplicar el método sobre f(x) con x0 = 3, γ = 0.001, tol=1e-12, maxit=1e5:\n",
    "x0 = 3\n",
    "gamma = 0.001\n",
    "tol = 1e-12\n",
    "maxit = int(1e5)\n",
    "\n",
    "result1 = gradient_descent(grad_f, x0, gamma, tol, maxit)\n",
    "print(\"Resultado 1:\", result1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Aplicar el método sobre f(x) con x0 = 3, γ = 0.01, tol=1e-12, maxit=1e5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado 2: -1.9999999999999882\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.01\n",
    "\n",
    "result2 = gradient_descent(grad_f, x0, gamma, tol, maxit)\n",
    "print(\"Resultado 2:\", result2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Contraste e interpretación de los resultados obtenidos en los apartados anteriores y comparación con los mínimos locales obtenidos analíticamente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante tener en cuenta que la función f(x) dada es una función unidimensional, por lo que solo tiene un mínimo local.\n",
    "\n",
    "En el resultado 1, utilizando γ = 0.001, se obtendrá una aproximación del mínimo local. El valor obtenido en este caso dependerá de la tolerancia establecida y del número máximo de iteraciones permitidas. Es posible que el resultado no sea exactamente el mínimo local, pero estará cerca de él.\n",
    "\n",
    "En el resultado 2, utilizando γ = 0.01, se espera una convergencia más rápida hacia el mínimo local. Esto se debe a que el tamaño de paso es mayor, lo que permite avanzar más rápidamente en la dirección del gradiente descendente. Sin embargo, si el tamaño de paso es demasiado grande, podría haber oscilaciones alrededor del mínimo y no alcanzar una convergencia precisa.\n",
    "\n",
    "Comparando los resultados obtenidos con el análisis analítico de f(x), se espera que el resultado se acerque al valor del mínimo local, que puede ser calculado derivando la función y resolviendo f'(x) = 0. Para la función dada, podemos calcular el mínimo local de la siguiente manera:\n",
    "\n",
    "f'(x) = 0\n",
    "12x^3 + 12x^2 - 24x = 0\n",
    "x(x^2 + x - 2) = 0\n",
    "\n",
    "Esto nos da tres posibles soluciones: x = 0, x = -2, x = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Aplicar nuevamente el método sobre f(x) con x0 = 3, γ = 0.1, tol=1e-12, maxit=1e5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado 2: -1.9999999999999882\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.01\n",
    "\n",
    "result2 = gradient_descent(grad_f, x0, gamma, tol, maxit)\n",
    "print(\"Resultado 2:\", result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
