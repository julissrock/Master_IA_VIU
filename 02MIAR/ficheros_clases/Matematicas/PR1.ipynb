{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tEn este ejercicio trabajaremos con el método de descenso de gradiente, el cual constituye otra herramienta crucial, en esta ocasión de la rama del cálculo, para el proceso de retropropagación asociado al entrenamiento de una red neuronal.\n",
    "\n",
    "a) Prográmese en Python el método de descenso de gradiente para funciones de\n",
    "n variables. La función deberá tener como parámetros de entradas:\n",
    "- El gradiente de la función que se desea minimizar ∇f (puede venir dada como otra función previamente implementada, grad f, con entrada un vector, representando el punto donde se quiere calcular el gradiente, y salida otro vector, representando\n",
    "el gradiente de f en dicho punto).\n",
    "- Un valor inicial x0 ∈ Rn (almacenado en un vector de n componentes). El ratio de aprendizaje γ (que se asume constante para cada iteración).\n",
    "- Un parámetro de tolerancia tol (con el que finalizar el proceso cuando ∥∇f (x)∥2 <\n",
    "tol).\n",
    "- Un número máximo de iteraciones maxit (con el fin de evitar ejecuciones indefinidas en caso de divergencia o convergencia muy lenta).\n",
    "- La salida de la función deberá ser la aproximación del x que cumple f ′(x) ≈ 0, correspondiente a la última iteración realizada en el método.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(grad_f, x0, gamma, tol, max_it):\n",
    "    x = x0\n",
    "    for i in range(int(max_it)):\n",
    "        gradient = grad_f(x)\n",
    "        norm_gradient = np.linalg.norm(gradient, 2)\n",
    "        if norm_gradient < tol:\n",
    "            break\n",
    "        x = x - gamma * gradient\n",
    "       \n",
    "    print(\"La iteración en la que finaliza el proceso es:\", i)\n",
    "    print(\"La aproximación alcanzada ha sido:\", norm_gradient)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "# # Parámetros de entrada\n",
    "# x0 = np.array([-1.0, 1.0])  # Vctor inicial\n",
    "# gamma = 0.01  # Tasa de aprendizaje\n",
    "# tol = 1e-12  # Tolerancia\n",
    "# max_it = 1e5  # Número máximo de iteraciones\n",
    "\n",
    "# # Aplicación del método de descenso de gradiente con los parámetros de entrada\n",
    "\n",
    "# result = gradient_descent(grad_f, x0, gamma, tol, max_it)\n",
    "\n",
    "# print(\"Punto de mínimo encontrado:\", result)\n",
    "# print(\"Valor de la función en el punto de mínimo:\", objective_function(result))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\tSea la función $ f : ℝ ⟶ ℝ $ dada por $ f (x) = 3x4 + 4x3 − 12x2 + 7$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b.1. Aplica el método sobre $ f(x) con x0 = 3 γ = 0.001, tol=1e-12, maxit=1e5 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La iteración en la que finaliza el proceso es: 831\n",
      "La aproximación alcanzada ha sido: 9.912071163853398e-13\n",
      "Resultado 2b1: [1.]\n"
     ]
    }
   ],
   "source": [
    "# Definir la función objetivo de n variables\n",
    "def objective_function(x):\n",
    "    return 3*x[0]**4 + 4*x[0]**3 - 12*x[0]**2 + 7\n",
    "\n",
    "# Definir la función gradiente de n variables\n",
    "def grad_f(x):\n",
    "    return np.array([12 * x[0]**3 + 12*x[0]**2 - 24*x[0]])\n",
    "\n",
    "# Aplicar el método sobre f(x) con x0 = 3, γ = 0.001, tol=1e-12, maxit=1e5:\n",
    "x0 = np.array([3.0])\n",
    "gamma = 0.001\n",
    "tol = 1e-12\n",
    "max_it = 1e5\n",
    "\n",
    "result2b1 = gradient_descent(grad_f, x0, gamma, tol, max_it)\n",
    "print(\"Resultado 2b1:\", result2b1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b.2. Aplicar el método sobre:\n",
    "$ f(x) con x0 = 3, γ = 0.1, tol=1e-12, maxit=1e5:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La iteración en la que finaliza el proceso es: 31\n",
      "La aproximación alcanzada ha sido: 8.384404281969182e-13\n",
      "Resultado 2b2: [-2.]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([3.0])\n",
    "gamma = 0.01\n",
    "tol = 1e-12\n",
    "max_it = 1e5\n",
    "\n",
    "result2b2 = gradient_descent(grad_f, x0, gamma, tol, max_it)\n",
    "print(\"Resultado 2b2:\", result2b2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b.3. Contraste e interpretación de los resultados obtenidos en los apartados anteriores y comparación con los mínimos locales obtenidos analíticamente:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función $f(x)$ dada es una función polinómica de una variable. Para calcular sus puntos críticos deberemos derivar buscar las soluciones a la derivada de la función una vez igualada a cero, $f'(x) = 0$.\n",
    "\n",
    "Una vez obtenidos los puntos críticos deberemos determinar si se tratan de un mínimo local, un máximo local o un punto de silla (o punto de inflexión en funciones de una variable). Este análisis se hará sustituyendo los puntos críticos encontrados en la función derivada segunda, de tal manera que:\n",
    "    $ si f''(x0) < 0$, entonces x0 es un máximo.\n",
    "    $ si f''(x0) > 0$, entonces x0 es un mínimo.\n",
    "    $ si f''(x0) = 0$, entonces x0 es un punto de silla (punto de inflexión).\n",
    "\n",
    "$$\n",
    "f'(x) = 0\n",
    "$$\n",
    "$$\n",
    "12x^3 + 12x^2 - 24x = 0\n",
    "$$\n",
    "$$\n",
    "x(x^2 + x - 2) = 0\n",
    "$$\n",
    "\n",
    "Esto nos da tres posibles soluciones: \n",
    "$$\n",
    "x1 = 0, x2 = -2, x3 = 1.\n",
    "$$\n",
    "\n",
    "\n",
    "Ahora calculamos la segunda derivada de la función, $f''(x)$:\n",
    "$$\n",
    "f''(x) = 36x^2 + 24x - 24 = 3x^2 + 2x - 2\n",
    "\n",
    "$$\n",
    "\n",
    "Y analizamos el valor de esta derivada segunda para cada uno de los puntos críticos: \n",
    "$$\n",
    "f''(0) = - 2 < 0, \n",
    "$$\n",
    "por lo tanto x1 = 0 es máximo\n",
    "$$\n",
    "f''(-2) = 6 > 0, \n",
    "$$\n",
    "por lo tanto x2 = -2 es mínimo\n",
    "$$\n",
    "f''(-1) = 3 > 0, \n",
    "$$\n",
    "por lo tanto x3 = 1 es mínimo\n",
    "\n",
    "En el resultado 1 obtenido mediante el progama de phyton, utilizando γ = 0.001, se ha obtenido una aproximación a uno de los mínimos locales obtenidos analíticamente, concretamente al punto crítico x3 = 1. En este caso, al partir de una tasa de aprendizaje baja, γ = 0.001, la función ha necesitado de 831 iteraciones para obtener el valor mínimo con la tolerancia exigida. El valor obtenido dependerá de la tolerancia establecida y del número máximo de iteraciones permitidas. Es posible que el resultado no sea exactamente el mínimo local, pero estará muy cerca de él.\n",
    "\n",
    "En el resultado 2, utilizando γ = 0.01, se ha obtenido una aproximación a otro de los mínimos locales obtenidos analíticamente, en este caso al punto crítico x2 = -2. Al partir de una tasa de aprendizaje mayor respecto al resultado 1, se espera una convergencia más rápida en la dirección del gradiente descendente hacia el mínimo local. Efectivamente, en este caso concreto, podemos comprobar que la función ha necesitado de sólo 31 iteraciones para alcanzar este mínimo local con la tolerancia exigida. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b.4. Aplicar nuevamente el método sobre $ f(x)$  e interpreta el resultado, con:\n",
    "$$\n",
    "x0 = 3$$\n",
    "$$ γ = 0.1 $$\n",
    "$$ tol = 1e-12 $$\n",
    "$$ maxit = 1e5\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julis\\AppData\\Local\\Temp\\ipykernel_19372\\4197698118.py:7: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return np.array([12 * x[0]**3 + 12*x[0]**2 - 24*x[0]])\n",
      "C:\\Users\\julis\\AppData\\Local\\Temp\\ipykernel_19372\\4197698118.py:7: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.array([12 * x[0]**3 + 12*x[0]**2 - 24*x[0]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La iteración en la que finaliza el proceso es: 99999\n",
      "La aproximación alcanzada ha sido: nan\n",
      "Resultado 2b4: [nan]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([3.0])\n",
    "gamma = 0.1\n",
    "tol = 1e-12\n",
    "max_it = 1e5\n",
    "\n",
    "result2b4 = gradient_descent(grad_f, x0, gamma, tol, max_it)\n",
    "print(\"Resultado 2b4:\", result2b4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado obtenido nos muestra que con una tasa de aprendizaje de γ = 0.1 el proceso no converge, alcanzándose el máximo número de iteraciones establecido. En este apartado, se ha empleado una tasa da aprendizaje demasiado grande, lo que hace que el descenso del gradiente vaya oscilando alrededor del mínimo y no se alcance nunca la convergencia o que ésta se alcance muy lentamente.\n",
    "\n",
    "Este resultado pone de manifiesto la importancia de elegir correctamente el valor de la tasa de aprendizaje , ya que si partimos de una demasiado baja el proceso convergerá lentamente necesitándose un elevado número de iteraciones para alcanzar el mínimo de la función (apartado 2.b.1). Si partimos de una tasa de aprendizaje demasiado alta puede ocurrir que no se alcance nunca la convergencia por lo ya argumentado (apartado 2.b.4).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b.5. Finalmente, aplica el método sobre $ f(x)$ con nuevamente el método sobre $f(x)$ e interpreta el resultado, con:\n",
    "$$x0 = 0$$\n",
    "$$ γ = 0.001 $$\n",
    "$$ tol = 1e-12 $$\n",
    "$$ maxit = 1e5 $$\n",
    "#### Interpreta el resultado y compáralo con el estudio analítico de f. Se trata de un resultado deseable? Por qué? A qué se debe este fenómeno?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La iteración en la que finaliza el proceso es: 0\n",
      "La aproximación alcanzada ha sido: 0.0\n",
      "Resultado 2b5: [0.]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([0.0])\n",
    "gamma = 0.001\n",
    "tol = 1e-12\n",
    "max_it = 1e5\n",
    "\n",
    "result2b5 = gradient_descent(grad_f, x0, gamma, tol, max_it)\n",
    "print(\"Resultado 2b5:\", result2b5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso vemos que la función no se ha ejecutado (proceso finalizado en iteración 0). El motivo es que el punto de origen seleccionado para inicializar el proceso iterativo del descenso del gradiente, x0, se trata de un punto crítico de la función. Conforme vimos en el apartado 2.b.3, el punto x= 0 es el punto crítico x1 del estudio analítico y concluimos que se trataba de un máximo local. Al ser un punto crítico, eso significa que el gradiente en este punto es 0, con lo que grad_f(x) de la función es 0. \n",
    "A su vez, la norma del vector gradiente será también 0 que cumple a su vez que es menor que la tolerancia establecia y por lo tanto, no hay ninguna iteración y no se llega a realizar ningún cálculo en la función.\n",
    "\n",
    "Esto se trata de un resultado indeseable ya que al encontrarnos ya en un punto crítico de la función el algoritmo del descenso del gradiente no se podría inicializar para determinar los mínimos al no producir ninguna variación incremental. \n",
    "\n",
    "Para tratar de solucionarlo, se podría modificar el código para que antes de comenzar a iterar se calculase si el gradiente en el punto de inicio es 0. En caso de que esto ocurra, se podría variar ligeramente el punto x0 para alejarlo ligeramente del punto crítico (sumándole o restándole una peque;a cantidad) y que de este modo el algoritmo pueda inicializar el proceso de cálculo.  \n",
    "\n",
    "A continuación se representa el código mejorado, donde en función del valor incremental que le demos, nos llevará a uno de los mínimos de la función. En este caso hemos optado por establecer un valor incremental de 0,0001 y nos lleva al mínimo local x=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La iteración en la que finaliza el proceso es: 941\n",
      "La aproximación alcanzada ha sido: 9.947598300641403e-13\n",
      "Resultado 2c2mod: [1.]\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent_v2(grad_f, x0, gamma, tol, max_it):\n",
    "    grad_f(x0)\n",
    "    norma_inic = np.linalg.norm(grad_f(x0), 2)\n",
    "    if norma_inic == 0:\n",
    "        for j in range(len(x0)):\n",
    "            x0[j] = x0[j] + 0.1\n",
    "    xv2 = x0\n",
    "    for i in range(int(max_it)):\n",
    "        gradient = grad_f(xv2)\n",
    "        norm_gradient = np.linalg.norm(gradient, 2)\n",
    "        if norm_gradient < tol:\n",
    "            break\n",
    "        xv2 = xv2 - gamma * gradient\n",
    "\n",
    "    print(\"La iteración en la que finaliza el proceso es:\", i)\n",
    "    print(\"La aproximación alcanzada ha sido:\", norm_gradient)\n",
    "    return xv2\n",
    "\n",
    "x0 = np.array([0.0])\n",
    "gamma = 0.001\n",
    "tol = 1e-12\n",
    "max_it = 1e5\n",
    "\n",
    "result2c2mod = gradient_descent_v2(grad_f, x0, gamma, tol, max_it)\n",
    "print(\"Resultado 2c2mod:\", result2c2mod)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.c. Sea la función $g: ℝ² ⟶ ℝ$  dada por $ g(x,y) = x^2 + y^3 + 3*x*y + 1 $\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.c.1 Aplíquese el método sobre g(x, y) con \n",
    "$$\n",
    "x0 = (-1, 1) $$\n",
    "$$ γ = 0.01 $$\n",
    "$$ tol = 1e-12 $$\n",
    "$$ maxit = 1e5\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La iteración en la que finaliza el proceso es: 31559\n",
      "La aproximación alcanzada ha sido: 9.99485293480103e-13\n",
      "Resultado 2c1: [-2.25  1.5 ]\n"
     ]
    }
   ],
   "source": [
    "# Definir la función objetivo de n variables\n",
    "def objective_function(x):\n",
    "    return x[0]**2 + x[1]**3 + 3*x[0]*x[1] + 1\n",
    "\n",
    "# Definir la función gradiente de n variables\n",
    "def grad_f(x):\n",
    "    return np.array([2 * x[0] + 3*x[1], 3*x[1]**2 + 3*x[0]])\n",
    "\n",
    "x0 = np.array([-1.0, 1.0])\n",
    "gamma = 0.001\n",
    "tol = 1e-12\n",
    "max_it = 1e5\n",
    "\n",
    "result2c1 = gradient_descent(grad_f, x0, gamma, tol, max_it)\n",
    "print(\"Resultado 2c1:\", result2c1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.c.2 Qué ocurre si ahora partimos de x0 = (0,0)? Se obtiene el resultado deseable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La iteración en la que finaliza el proceso es: 0\n",
      "La aproximación alcanzada ha sido: 0.0\n",
      "Resultado 2c2: [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([0.0, 0.0])\n",
    "gamma = 0.001\n",
    "tol = 1e-12\n",
    "max_it = 1e5\n",
    "\n",
    "result2c2 = gradient_descent(grad_f, x0, gamma, tol, max_it)\n",
    "print(\"Resultado 2c2:\", result2c2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se obtienen los resultados deseables porque nos encontramos en el mismo caso que el punto 2.b.5. Al tratarse el punto (0,0) de un punto crítico, la función de gradiente descendiente no puede comenzar a iterar porque su gradiente es el vector 0.\n",
    "\n",
    "Esto se soluciona implementando la función gradient_descen_v2 pero adaptada a varias variables ya que la variación incremental se tiene que dar a todas las variables de la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La iteración en la que finaliza el proceso es: 34273\n",
      "La aproximación alcanzada ha sido: 9.99485293480103e-13\n",
      "Resultado 2c2mod: [-2.25  1.5 ]\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent_v2(grad_f, x0, gamma, tol, max_it):\n",
    "    grad_f(x0)\n",
    "    norma_inic = np.linalg.norm(grad_f(x0), 2)\n",
    "    if norma_inic == 0:\n",
    "        for j in range(len(x0)):\n",
    "            x0[j] = x0[j] + 0.1\n",
    "    xv2 = x0\n",
    "    for i in range(int(max_it)):\n",
    "        gradient = grad_f(xv2)\n",
    "        norm_gradient = np.linalg.norm(gradient, 2)\n",
    "        if norm_gradient < tol:\n",
    "            break\n",
    "        xv2 = xv2 - gamma * gradient\n",
    "\n",
    "    print(\"La iteración en la que finaliza el proceso es:\", i)\n",
    "    print(\"La aproximación alcanzada ha sido:\", norm_gradient)\n",
    "    return xv2\n",
    "\n",
    "x0 = np.array([0.0, 0.0])\n",
    "gamma = 0.001\n",
    "tol = 1e-12\n",
    "max_it = 1e5\n",
    "\n",
    "result2c2mod = gradient_descent_v2(grad_f, x0, gamma, tol, max_it)\n",
    "print(\"Resultado 2c2mod:\", result2c2mod)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.c.3 Realícese el estudio analítico de la función y utilícese para explicar y contrastar los resultados obtenidos en los dos apartados anteriores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función $g(x,y)$ dada es una función de dos variables. Para calcular sus puntos críticos deberemos calcular las derivadas parciales respecto a cada variable e igualarlas a 0: $(\\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y})\n",
    " = (0,0)$.\n",
    " Con esto obtendremos los puntos críticos de la función $g(x,y)$\n",
    "\n",
    "Una vez obtenidos los puntos críticos deberemos determinar si se tratan de un mínimo local, un máximo local o un punto de silla. Este análisis se hará calculando la matriz Hessiana (matriz de derivadas parciales segundas):\n",
    "$$\n",
    "H = \\begin{bmatrix} \\frac{\\partial^2 g}{\\partial x^2} & \\frac{\\partial^2 g}{\\partial x  \\partial y} \\\\ \\frac{\\partial^2 g}{\\partial x  \\partial y} & \\frac{\\partial^2 g}{\\partial y^2} \\end{bmatrix}\n",
    "$$\n",
    "Posteriormente se calculará el valor del determinante de la Hessiana y analizaremos las distintas posibilidades según la teoría. \n",
    "\n",
    "En nuestro caso tenemos que:\n",
    "$$ (\\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y})\n",
    " = (2x + 3y , 3 y^2 + 3 x)=(0,0)$$\n",
    "Con lo que resolviendo el sistema de 2 ecuaciones con 2 incógnitas obtenemos los siguientes puntos críticos:\n",
    "$$\n",
    "(x0, y0) = (0,0)\n",
    "$$\n",
    "$$\n",
    "(x1, y1) = (2.25, 1.5)\n",
    "$$\n",
    "\n",
    "Ahora calculamos la matriz Hessiana definida anteriormente y obtenemos:\n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix} 2 & 3 \\\\ 3  & 6y \\end{bmatrix}\n",
    "$$\n",
    "Y calculamos el valor del determinante de la Hessiana para cada punto crítico obtenido:\n",
    " $$\n",
    "\n",
    "det(H(x0,y0)) = -9 < 0\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "det(H(x1,y1)) = 9 > 0\n",
    "$$\n",
    "Por lo tanto, para el punto crítico (x0,y0), tenemos que el det de la Hessiana en este punto es < 0, con lo que el punto (x0, y0)=(0,0) es un punto de silla.\n",
    "\n",
    "Por otro lado, en el otro punto crítico (x1, y1) tenemos que el det de la Hessiana es > 0, con lo que tenemos que el punto (x1, y1) = (2.25, 1.5) es un mínimo local, al ser el valor de la Hessiana h11 > 0\n",
    "\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
